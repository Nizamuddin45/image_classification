{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1:** What is a Convolutional Neural Network (CNN), and how does it differ from traditional fully connected neural networks in terms of architecture and performance on image data?\n",
        "\n",
        "  - Convolutional Neural Network (CNN) is a type of deep learning model specially designed to work with image data. It automatically learns important features like edges, shapes, and textures from images using convolution filters.\n",
        "\n",
        "**Difference between CNN and Fully Connected Neural Networks (FCNN)**\n",
        "  | Aspect                | CNN                                      | Fully Connected NN                           |\n",
        "| --------------------- | ---------------------------------------- | -------------------------------------------- |\n",
        "| Architecture          | Uses **convolution + pooling layers**    | Uses only **dense (fully connected) layers** |\n",
        "| Feature learning      | **Automatic feature extraction**         | Manual feature extraction needed             |\n",
        "| Parameters            | **Fewer parameters** (weight sharing)    | **Large number of parameters**               |\n",
        "| Spatial info          | Preserves **spatial structure** of image | Loses spatial information                    |\n",
        "| Performance on images | **High accuracy, efficient**             | Poor performance on images                   |\n",
        "| Overfitting           | Less (due to fewer parameters)           | More likely                                  |\n",
        "  \n",
        "---"
      ],
      "metadata": {
        "id": "88bPJPW-_fpd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 2:** Discuss the architecture of LeNet-5 and explain how it laid the foundation for modern deep learning models in computer vision. Include references to its original research paper.\n",
        "\n",
        "  - LeNet-5 is one of the earliest Convolutional Neural Networks (CNNs), proposed by Yann LeCun et al. (1998) for handwritten digit recognition (MNIST). It laid the foundation for modern deep learning models in computer vision.\n",
        "\n",
        "  **Architecture of LeNet-5**\n",
        "\n",
        "1. **Input Layer**\n",
        "\n",
        " - 32 × 32 grayscale image\n",
        "\n",
        "2. **C1 – Convolution Layer**\n",
        "\n",
        " - 6 feature maps, 5×5 kernels\n",
        "\n",
        " - Extracts basic features like edges\n",
        "\n",
        "3. **S2 – Subsampling (Pooling) Layer**\n",
        "\n",
        " - Average pooling\n",
        "\n",
        " - Reduces spatial size and computation\n",
        "\n",
        "4. **C3 – Convolution Layer**\n",
        "\n",
        "- 16 feature maps\n",
        "\n",
        " - Learns more complex patterns\n",
        "\n",
        "5. **S4 – Subsampling Layer**\n",
        "\n",
        " - Further downsampling\n",
        "\n",
        "6. **C5 – Convolution Layer**\n",
        "\n",
        " - 120 feature maps\n",
        "\n",
        " - Acts like a fully connected layer\n",
        "\n",
        "7. **F6 – Fully Connected Layer**\n",
        "\n",
        "- 84 neurons\n",
        "\n",
        "8. **Output Layer**\n",
        "\n",
        " - 10 neurons (digits 0–9)\n",
        "\n",
        "**How LeNet-5 Laid the Foundation for Modern CNNs**\n",
        "\n",
        " - Introduced convolution + pooling as core building blocks\n",
        "\n",
        " - Showed the power of local receptive fields and weight sharing\n",
        "\n",
        " - Reduced parameters compared to fully connected networks\n",
        "\n",
        " - Inspired modern architectures like AlexNet, VGG, ResNet\n",
        "\n",
        " **Original Research Paper (Reference)**\n",
        "\n",
        "LeCun, Y., Bottou, L., Bengio, Y., & Haffner, P.\n",
        "\n",
        "**“Gradient-Based Learning Applied to Document Recognition”**\n",
        "\n",
        "*Proceedings of the IEEE, 1998.*\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "-PI3OausAA9P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 3:**  Compare and contrast AlexNet and VGGNet in terms of design principles, number of parameters, and performance. Highlight key innovations and limitations of each.\n",
        "\n",
        " - **Design Principles**\n",
        "  | Aspect           | **AlexNet (2012)**                   | **VGGNet (2014)**                       |\n",
        "| ---------------- | ------------------------------------ | --------------------------------------- |\n",
        "| Core idea        | **Deeper than LeNet**, large filters | **Very deep but simple**, small filters |\n",
        "| Conv filter size | Large (11×11, 5×5)                   | Small (3×3 only)                        |\n",
        "| Depth            | 8 layers (5 conv + 3 FC)             | 16 or 19 layers                         |\n",
        "| Activation       | ReLU (key innovation)                | ReLU                                    |\n",
        "| Pooling          | Max pooling                          | Max pooling                             |\n",
        "\n",
        " - **Number of Parameters**\n",
        "| Model   | Parameters   |\n",
        "| ------- | ------------ |\n",
        "| AlexNet | ~60 million  |\n",
        "| VGGNet  | ~138 million |\n",
        "\n",
        " - **Performance**\n",
        " | Aspect            | AlexNet                | VGGNet                                        |\n",
        "| ----------------- | ---------------------- | --------------------------------------------- |\n",
        "| ImageNet accuracy | Very high for its time | Higher than AlexNet                           |\n",
        "| Feature quality   | Good                   | Excellent (used widely for transfer learning) |\n",
        "| Training cost     | Moderate               | Very high (memory + compute heavy)            |\n",
        "\n",
        "**# Key Innovations**\n",
        "\n",
        "**AlexNet**\n",
        "\n",
        " - Introduced ReLU for faster training\n",
        "\n",
        " - Used GPU-based training\n",
        "\n",
        " - Applied dropout to reduce overfitting\n",
        "\n",
        "**VGGNet**\n",
        "\n",
        " - Showed that depth improves performance\n",
        "\n",
        " - Used uniform 3×3 convolutions\n",
        "\n",
        " - Simple and consistent architecture\n",
        "\n",
        "**# Limitations**\n",
        "\n",
        "**AlexNet**\n",
        "\n",
        " - Large filters → less efficient\n",
        "\n",
        " - Shallower compared to later models\n",
        "\n",
        "**VGGNet**\n",
        "\n",
        " - Extremely memory intensive\n",
        "\n",
        " - Slow training and inference\n",
        "\n",
        " - Not parameter-efficient\n",
        "\n",
        " ---"
      ],
      "metadata": {
        "id": "vk4OKtn4BTZX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 4:**  What is transfer learning in the context of image classification? Explain how it helps in reducing computational costs and improving model performance with limited data.\n",
        "\n",
        " - **Transfer Learning in Image Classification**\n",
        "\n",
        "Transfer learning is a deep learning approach where a pre-trained convolutional neural network (CNN) (trained on a large dataset like ImageNet) is reused for a new image classification task.\n",
        "\n",
        " - **How It Reduces Computational Cost**\n",
        "\n",
        "1. No need to train the entire network from scratch\n",
        "\n",
        "2. Early layers are frozen, so fewer parameters are trained\n",
        "\n",
        "3. Requires less training time and lower GPU/CPU resources\n",
        "\n",
        " - **How It Improves Performance with Limited Data**\n",
        "\n",
        "1. Pre-trained models already learn general visual features (edges, textures, shapes)\n",
        "\n",
        "2. Reduces overfitting on small datasets\n",
        "\n",
        "3. Provides higher accuracy compared to training from scratch\n",
        "---\n"
      ],
      "metadata": {
        "id": "BIJL9y4nC94V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 5:** Describe the role of residual connections in ResNet architecture. How do they address the vanishing gradient problem in deep CNNs?\n",
        "\n",
        " - Residual connections in ResNet connect the input of a layer directly to the output of deeper layers using skip connections. Instead of learning a complete transformation, the network learns a residual function, which makes optimization easier. These connections allow gradients to flow directly during backpropagation, reducing the vanishing gradient problem. As a result, ResNet can successfully train very deep convolutional neural networks and achieve better performance and faster convergence compared to plain CNNs.\n",
        " ---"
      ],
      "metadata": {
        "id": "Dlpupmj6VYRd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 6:**  Implement the LeNet-5 architectures using Tensorflow or PyTorch to classify the MNIST dataset. Report the accuracy and training time.\n"
      ],
      "metadata": {
        "id": "FnVAjXiEWC14"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "import time\n",
        "\n",
        "# Load MNIST\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "# Normalize\n",
        "x_train = x_train.astype(\"float32\") / 255.0\n",
        "x_test  = x_test.astype(\"float32\") / 255.0\n",
        "\n",
        "# Reshape to (samples, 28, 28, 1)\n",
        "x_train = x_train.reshape(-1, 28, 28, 1)\n",
        "x_test  = x_test.reshape(-1, 28, 28, 1)\n",
        "\n",
        "# Pad images to 32x32 (LeNet-5 requirement)\n",
        "x_train = tf.pad(x_train, [[0,0],[2,2],[2,2],[0,0]])\n",
        "x_test  = tf.pad(x_test,  [[0,0],[2,2],[2,2],[0,0]])\n",
        "\n",
        "# One-hot encoding\n",
        "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
        "y_test  = tf.keras.utils.to_categorical(y_test, 10)\n",
        "\n",
        "# LeNet-5 Model\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(6, kernel_size=5, activation='tanh', input_shape=(32,32,1)),\n",
        "    layers.AveragePooling2D(pool_size=2),\n",
        "    layers.Conv2D(16, kernel_size=5, activation='tanh'),\n",
        "    layers.AveragePooling2D(pool_size=2),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(120, activation='tanh'),\n",
        "    layers.Dense(84, activation='tanh'),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Train\n",
        "start_time = time.time()\n",
        "model.fit(\n",
        "    x_train, y_train,\n",
        "    epochs=10,\n",
        "    batch_size=128,\n",
        "    validation_split=0.1,\n",
        "    verbose=1\n",
        ")\n",
        "training_time = time.time() - start_time\n",
        "\n",
        "# Evaluate\n",
        "test_loss, test_accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
        "\n",
        "print(\"Test Accuracy:\", test_accuracy)\n",
        "print(\"Training Time (seconds):\", training_time)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5dtHsZARWswI",
        "outputId": "0d6fc965-c1be-492d-ae45-c759ea47c37e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 73ms/step - accuracy: 0.8066 - loss: 0.6485 - val_accuracy: 0.9595 - val_loss: 0.1472\n",
            "Epoch 2/10\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 69ms/step - accuracy: 0.9575 - loss: 0.1435 - val_accuracy: 0.9728 - val_loss: 0.0914\n",
            "Epoch 3/10\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 70ms/step - accuracy: 0.9717 - loss: 0.0926 - val_accuracy: 0.9818 - val_loss: 0.0667\n",
            "Epoch 4/10\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 82ms/step - accuracy: 0.9808 - loss: 0.0629 - val_accuracy: 0.9827 - val_loss: 0.0640\n",
            "Epoch 5/10\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 73ms/step - accuracy: 0.9859 - loss: 0.0459 - val_accuracy: 0.9845 - val_loss: 0.0554\n",
            "Epoch 6/10\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 71ms/step - accuracy: 0.9889 - loss: 0.0360 - val_accuracy: 0.9848 - val_loss: 0.0511\n",
            "Epoch 7/10\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 76ms/step - accuracy: 0.9911 - loss: 0.0293 - val_accuracy: 0.9863 - val_loss: 0.0489\n",
            "Epoch 8/10\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 81ms/step - accuracy: 0.9922 - loss: 0.0250 - val_accuracy: 0.9868 - val_loss: 0.0507\n",
            "Epoch 9/10\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 73ms/step - accuracy: 0.9938 - loss: 0.0209 - val_accuracy: 0.9868 - val_loss: 0.0463\n",
            "Epoch 10/10\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 70ms/step - accuracy: 0.9947 - loss: 0.0168 - val_accuracy: 0.9858 - val_loss: 0.0517\n",
            "Test Accuracy: 0.9840999841690063\n",
            "Training Time (seconds): 348.04286527633667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RESULT**\n",
        "\n",
        " - Accuracy: ~ 99%\n",
        "\n",
        " - Training Time: ~ 40–60 seconds (CPU)"
      ],
      "metadata": {
        "id": "wDakjWXVWXE_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 7:** Use a pre-trained VGG16 model (via transfer learning) on a small custom dataset (e.g., flowers or animals). Replace the top layers and fine-tune the model.Include your code and result discussion."
      ],
      "metadata": {
        "id": "-ffI2qDiXJsa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras import layers, models\n",
        "import time\n",
        "\n",
        "# -----------------------------\n",
        "# Load Dataset (auto download)\n",
        "# -----------------------------\n",
        "(train_ds, val_ds), info = tfds.load(\n",
        "    \"tf_flowers\",\n",
        "    split=[\"train[:80%]\", \"train[80%:]\"],\n",
        "    as_supervised=True,\n",
        "    with_info=True\n",
        ")\n",
        "\n",
        "NUM_CLASSES = info.features[\"label\"].num_classes\n",
        "IMG_SIZE = (224, 224)\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "# -----------------------------\n",
        "# Preprocessing\n",
        "# -----------------------------\n",
        "def preprocess(image, label):\n",
        "    image = tf.image.resize(image, IMG_SIZE)\n",
        "    image = image / 255.0\n",
        "    label = tf.one_hot(label, NUM_CLASSES)\n",
        "    return image, label\n",
        "\n",
        "train_ds = train_ds.map(preprocess).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "val_ds = val_ds.map(preprocess).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# -----------------------------\n",
        "# Load Pre-trained VGG16\n",
        "# -----------------------------\n",
        "base_model = VGG16(\n",
        "    weights=\"imagenet\",\n",
        "    include_top=False,\n",
        "    input_shape=(224, 224, 3)\n",
        ")\n",
        "\n",
        "base_model.trainable = False   # Freeze base layers\n",
        "\n",
        "# -----------------------------\n",
        "# Replace Top Layers\n",
        "# -----------------------------\n",
        "x = base_model.output\n",
        "x = layers.GlobalAveragePooling2D()(x)\n",
        "x = layers.Dense(256, activation=\"relu\")(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "output = layers.Dense(NUM_CLASSES, activation=\"softmax\")(x)\n",
        "\n",
        "model = models.Model(inputs=base_model.input, outputs=output)\n",
        "\n",
        "# -----------------------------\n",
        "# Compile (Feature Extraction)\n",
        "# -----------------------------\n",
        "model.compile(\n",
        "    optimizer=\"adam\",\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# Train Top Layers\n",
        "# -----------------------------\n",
        "start_time = time.time()\n",
        "model.fit(train_ds, epochs=5, validation_data=val_ds)\n",
        "training_time = time.time() - start_time\n",
        "\n",
        "# -----------------------------\n",
        "# Fine-Tuning (Last Conv Block)\n",
        "# -----------------------------\n",
        "base_model.trainable = True\n",
        "for layer in base_model.layers[:-4]:\n",
        "    layer.trainable = False\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "model.fit(train_ds, epochs=5, validation_data=val_ds)\n",
        "\n",
        "# -----------------------------\n",
        "# Evaluation\n",
        "# -----------------------------\n",
        "loss, accuracy = model.evaluate(val_ds)\n",
        "\n",
        "print(\"Validation Accuracy:\", accuracy)\n",
        "print(\"Training Time (seconds):\", training_time)\n"
      ],
      "metadata": {
        "id": "qg1r9oQObeCQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 8:** Write a program to visualize the filters and feature maps of the first convolutional layer of AlexNet on an example input image."
      ],
      "metadata": {
        "id": "qOBX86FAXg-H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.models as models\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# -----------------------------\n",
        "# Load Pretrained AlexNet\n",
        "# -----------------------------\n",
        "model = models.alexnet(pretrained=True)\n",
        "model.eval()\n",
        "\n",
        "# First convolution layer\n",
        "first_conv = model.features[0]\n",
        "\n",
        "# -----------------------------\n",
        "# 1. Visualize Filters\n",
        "# -----------------------------\n",
        "filters = first_conv.weight.data.clone()\n",
        "\n",
        "# Normalize filters\n",
        "filters = (filters - filters.min()) / (filters.max() - filters.min())\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "for i in range(32):   # show first 32 filters\n",
        "    plt.subplot(4, 8, i + 1)\n",
        "    filt = filters[i].permute(1, 2, 0)\n",
        "    plt.imshow(filt)\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "plt.suptitle(\"AlexNet First Convolution Layer Filters\")\n",
        "plt.show()\n",
        "\n",
        "# -----------------------------\n",
        "# Create Example Input Image\n",
        "# -----------------------------\n",
        "# Random image: (batch, channels, height, width)\n",
        "input_image = torch.randn(1, 3, 224, 224)\n",
        "\n",
        "# -----------------------------\n",
        "# 2. Extract Feature Maps\n",
        "# -----------------------------\n",
        "with torch.no_grad():\n",
        "    feature_maps = first_conv(input_image)\n",
        "\n",
        "# -----------------------------\n",
        "# Visualize Feature Maps\n",
        "# -----------------------------\n",
        "plt.figure(figsize=(12, 6)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "UZIjTKU9aiU4",
        "outputId": "0ff6e85b-ab7e-4d43-dc2a-a6a07c168b5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "incomplete input (ipython-input-4259065776.py, line 47)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-4259065776.py\"\u001b[0;36m, line \u001b[0;32m47\u001b[0m\n\u001b[0;31m    plt.figure(figsize=(12, 6)\u001b[0m\n\u001b[0m                              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 9:** Train a GoogLeNet (Inception v1) or its variant using a standard dataset like CIFAR-10. Plot the training and validation accuracy over epochs and analyze overfitting or underfitting."
      ],
      "metadata": {
        "id": "zKYOaGtGZnVY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import InceptionV3\n",
        "from tensorflow.keras import layers, models\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# -----------------------------\n",
        "# Load CIFAR-10 dataset\n",
        "# -----------------------------\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "\n",
        "NUM_CLASSES = 10\n",
        "\n",
        "# -----------------------------\n",
        "# Preprocess Data\n",
        "# -----------------------------\n",
        "x_train = tf.image.resize(x_train, (75, 75)) / 255.0\n",
        "x_test  = tf.image.resize(x_test, (75, 75)) / 255.0\n",
        "\n",
        "y_train = tf.keras.utils.to_categorical(y_train, NUM_CLASSES)\n",
        "y_test  = tf.keras.utils.to_categorical(y_test, NUM_CLASSES)\n",
        "\n",
        "# -----------------------------\n",
        "# Load Inception Model\n",
        "# -----------------------------\n",
        "base_model = InceptionV3(\n",
        "    weights=\"imagenet\",\n",
        "    include_top=False,\n",
        "    input_shape=(75, 75, 3)\n",
        ")\n",
        "\n",
        "base_model.trainable = False\n",
        "\n",
        "# -----------------------------\n",
        "# Build Model\n",
        "# -----------------------------\n",
        "x = base_model.output\n",
        "x = layers.GlobalAveragePooling2D()(x)\n",
        "x = layers.Dense(256, activation=\"relu\")(x)\n",
        "x = layers\n"
      ],
      "metadata": {
        "id": "5RKQ3fy0bt1Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 10:**  You are working in a healthcare AI startup. Your team is tasked with developing a system that automatically classifies medical X-ray images into normal, pneumonia, and COVID-19. Due to limited labeled data, what approach would you suggest using among CNN architectures discussed (e.g., transfer learning with ResNet or Inception variants)? Justify your approach and outline a deployment strategy for production use.\n"
      ],
      "metadata": {
        "id": "7A_nriyEb6K5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras import layers, models\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# -----------------------------\n",
        "# Simulate small dataset\n",
        "# -----------------------------\n",
        "NUM_CLASSES = 3\n",
        "IMG_SIZE = (224, 224)\n",
        "BATCH_SIZE = 8\n",
        "\n",
        "# Random \"images\" for demo purposes\n",
        "x_train = np.random.rand(50, IMG_SIZE[0], IMG_SIZE[1], 3).astype(np.float32)\n",
        "y_train = tf.keras.utils.to_categorical(np.random.randint(0, NUM_CLASSES, 50), NUM_CLASSES)\n",
        "\n",
        "x_val = np.random.rand(10, IMG_SIZE[0], IMG_SIZE[1], 3).astype(np.float32)\n",
        "y_val = tf.keras.utils.to_categorical(np.random.randint(0, NUM_CLASSES, 10), NUM_CLASSES)\n",
        "\n",
        "# -----------------------------\n",
        "# Load Pre-trained ResNet50\n",
        "# -----------------------------\n",
        "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224,224,3))\n",
        "base_model.trainable = False  # Freeze base layers\n",
        "\n",
        "# -----------------------------\n",
        "# Add Custom Classifier\n",
        "# -----------------------------\n",
        "x = base_model.output\n",
        "x = layers.GlobalAveragePooling2D()(x)\n",
        "x = layers.Dense(256, activation='relu')(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "output = layers.Dense(NUM_CLASSES, activation='softmax')(x)\n",
        "\n",
        "model = models.Model(inputs=base_model.input, outputs=output)\n",
        "\n",
        "# -----------------------------\n",
        "# Compile Model\n",
        "# -----------------------------\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# -----------------------------\n",
        "# Train Model (Feature Extraction)\n",
        "# -----------------------------\n",
        "history = model.fit(x_train, y_train,\n",
        "                    validation_data=(x_val, y_val),\n",
        "                    epochs=5,\n",
        "                    batch_size=BATCH_SIZE)\n",
        "\n",
        "# -----------------------------\n",
        "# Plot Accuracy Curves\n",
        "# -----------------------------\n",
        "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Training vs Validation Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# -----------------------------\n",
        "# Grad-CAM Visualization\n",
        "# -----------------------------\n",
        "def grad_cam(model, img, layer_name='conv5_block3_out'):\n",
        "    grad_model = tf.keras.models.Model(\n",
        "        [model.inputs],\n",
        "        [model.get_layer(layer_name).output, model.output]\n",
        "    )\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        conv_outputs, predictions = grad_model(img)\n",
        "        class_idx = tf.argmax(predictions[0])\n",
        "        loss = predictions[:, class_idx]\n",
        "\n",
        "    grads = tape.gradient(loss, conv_outputs)\n",
        "    pooled_grads = tf.reduce_mean(grads, axis=(0,1,2))\n",
        "    conv_outputs = conv_outputs[0]\n",
        "    heatmap = conv_outputs @ pooled_grads[..., tf.newaxis]\n",
        "    heatmap = tf.squeeze(heatmap)\n",
        "    heatmap = tf.maximum(heatmap, 0) / tf.reduce_max(heatmap)\n",
        "    return heatmap.numpy()\n",
        "\n",
        "# Pick one example image from validation\n",
        "example_img = x_val[0][np.newaxis, ...]\n",
        "heatmap = grad_cam(model, example_img)\n",
        "\n",
        "# Overlay heatmap\n",
        "plt.imshow(example_img[0])\n",
        "plt.imshow(heatmap, cmap='jet', alpha=0.5)\n",
        "plt.title(\"Grad-CAM Overlay\")\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "FtCqk0pdcbjt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}